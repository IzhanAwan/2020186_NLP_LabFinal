# -*- coding: utf-8 -*-
"""2020186.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/182qzQdEyBmoIIOQNkdXfAPVR714L3D8x
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

plt.rc('figure',figsize=(17,13))
import plotly.express as px
import plotly.graph_objs as go
import plotly.offline as pyo
from plotly.subplots import make_subplots
pyo.init_notebook_mode()

import re
import string 

import nltk
from nltk.probability import FreqDist
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk import pos_tag
from nltk.tokenize import word_tokenize

from wordcloud import WordCloud
from tqdm.auto import tqdm

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import re
import string 
from tqdm.auto import tqdm

import nltk
from nltk.probability import FreqDist
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk import pos_tag
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

df=pd.read_csv(r'/content/drive/MyDrive/tweets.csv')
df.head(10)

print('Rows              :',df.shape[0])
print('Columns           :',df.shape[1])
print('\nFeatures        :\n',df.columns.tolist())
print('\nMissing values  :',df.isna().sum().values.sum())
print('\nUnique values   :',df.nunique())

# Reducing the data
df = df.drop_duplicates() 
df = df.dropna()
print('Rows              :',df.shape[0])
print('\nMissing values  :',df.isna().sum().values.sum())

df = df.head(10000)

df.head(5)

def remove_line_breaks(text):
    text = text.replace('\r', ' ').replace('\n', ' ')
    return text

def remove_punctuation(text):
    re_replacements = re.compile("__[A-Z]+__")  # such as __NAME__, __LINK__
    re_punctuation = re.compile("[%s]" % re.escape(string.punctuation))
    '''Escape all the characters in pattern except ASCII letters and numbers'''
    tokens = word_tokenize(text)
    tokens_zero_punctuation = []
    for token in tokens:
        if not re_replacements.match(token):
            token = re_punctuation.sub(" ", token)
        tokens_zero_punctuation.append(token)
    return ' '.join(tokens_zero_punctuation)

def remove_special_characters(text):
    text = re.sub('[^a-zA-z0-9\s]', '', text)
    return text

def lowercase(text):
    text_low = [token.lower() for token in word_tokenize(text)]
    return ' '.join(text_low)

def remove_stopwords(text):
    stop = set(stopwords.words('english'))
    word_tokens = nltk.word_tokenize(text)
    text = " ".join([word for word in word_tokens if word not in stop])
    return text

def remove_one_character_words(text):
    '''Remove words from dataset that contain only 1 character'''
    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      
    return ' '.join(text_high_use)   
    
def stem(text):
    stemmer = nltk.stem.snowball.SnowballStemmer('english')
    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        
    return ' '.join(text_stemmed)

def lemma(text):
    wordnet_lemmatizer = WordNetLemmatizer()
    word_tokens = nltk.word_tokenize(text)
    text_lemma = " ".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       
    return ' '.join(text_lemma)

def sentence_word(text):
    word_tokens = nltk.word_tokenize(text)
    return word_tokens

def paragraph_sentence(text):
    sent_token = nltk.sent_tokenize(text)
    return sent_token    

def tokenize(text):
    """Return a list of words in a text."""
    return re.findall(r'\w+', text)

def remove_numbers(text):
    no_nums = re.sub(r'\d+', '', text)
    return ''.join(no_nums)

def clean_text(text):
    _steps = [
    remove_line_breaks,
    remove_one_character_words,
    remove_special_characters,
    lowercase,
    remove_punctuation,
    remove_stopwords,
    stem,
    remove_numbers
]
    for step in _steps:
        text=step(text)
    return text

text = df["text"].to_string()
text

import nltk
nltk.download('punkt')
nltk.download('stopwords')

df["text"] = df["text"].astype(str)
df["text"] = [x.replace(':',' ') for x in df["text"]]
df['clean_text'] = pd.Series([clean_text(i) for i in tqdm(df['text'])])

cleaned = clean_text(text) 
cleaned

wordsarray = df["clean_text"].values
wordsarray

listt = []

for i in wordsarray:
    listt.append(str(i))
listt[1:10]

"""## EDA"""

df.head(5)

from collections import defaultdict

# # To check 30 most used Hashtags
# df['hashtags']=df['hashtags'].fillna('[]')

# all_hashtags=[]
# for i in range(len(df['hashtags'])):
#     a=df['hashtags'][i].strip('][').split(',')
#     for i in a:
#         all_hashtags.append(i)
# all_hashtags=['No hashtags' if x=='' else x for x in all_hashtags]

# all_hashtags=pd.Series(np.array(all_hashtags))
# print('There are {} instances of tweets in which No hashtags were used'.format(all_hashtags.value_counts()[1]))

# common_hashtags=all_hashtags.value_counts().drop(labels='No hashtags')[:30].rename_axis('Common Hashtags').reset_index(name='count')
# fig=px.treemap(common_hashtags,path=['Common Hashtags'],values='count',title='Top 30 Common Hashtags')
# fig.show()



"""## N-Gram Analysis & Word Cloud"""

tokens = nltk.word_tokenize(cleaned)
bigrams = nltk.bigrams(tokens)
frequence = nltk.FreqDist(bigrams)
for key,value in frequence.items():
    print(key,value)

from nltk import ngrams

def n_gram(n):                                             
    n_grams = ngrams(text.split(), n)
    return n_grams

sent = []
df_freq = []

def frequency(grammed):
    sent.clear()
    df_freq.clear()
    freq = nltk.FreqDist(grammed)
    for k, v in freq.items():
        sent.append(k)                     # Sentences is a list, stores the grams(ignores duplicates)
        df_freq.append(v)

def create_plot(num):
    frequency(n_gram(num)) # Send "num" parameter to "n_gram func." and send the result to "frequency func."

    gram_frame = pd.DataFrame(sent)       # gram_frame is the data frame to store grams and freq.

    gram_frame['frequencies'] = df_freq
    if num == 2:
        gram_frame.columns = ['first', 'second', 'frequencies']
    if num == 3:
        gram_frame.columns = ['first', 'second', 'third', 'frequencies']

    gram_frame.sort_values("frequencies", axis=0, ascending=False, inplace=True, na_position='last')

    gram_frame = gram_frame.head(20)            # Only take the top 20 of gram_frame

    total = sum(df_freq)

    gram_frame["ratio"] = gram_frame['frequencies'].div(total)   # Additional, ratio is added

    plt.rcdefaults()
    fig, ax = plt.subplots()

    if num == 2:
        grams = gram_frame["first"] + " " + gram_frame["second"]
    if num == 3:
        grams = gram_frame["first"] + " " + gram_frame["second"] + " " + gram_frame["third"]

    # Create plot
    y_pos = np.arange(len(grams))
    performance = gram_frame["frequencies"]

    ax.barh(y_pos, performance)
    ax.set_yticks(y_pos)
    ax.set_yticklabels(grams)
    ax.invert_yaxis()  # labels read top-to-bottom
    ax.set_xlabel('Frequency')
    ax.set_title('n grams')

    plt.show()
    display(gram_frame)

create_plot(2)

create_plot(3)

plt.figure(figsize=(16,13))
wc=WordCloud(background_color='lightblue',colormap='Set2',max_words=1000,max_font_size=200,width=1600,height=800)
wc.generate(" ".join(listt))
plt.title('Word Cloud',fontsize=20)
plt.imshow(wc.recolor(colormap='Set2',random_state=17),alpha=0.98,interpolation='bilinear')
plt.axis('off')

"""## 4. Sentimental Analysis"""

!pip install textblob

pip install -U git+https://github.com/sloria/TextBlob.git@dev

from textblob import TextBlob

# Perform sentiment analysis on each tweet
polarity_scores = []
for text in cleaned:
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    polarity_scores.append(polarity)

# Plot the sentiment scores as a bar chart
fig, ax = plt.subplots()
ax.bar(['Negative', 'Neutral', 'Positive'], [
       len(list(filter(lambda x: x < 0, polarity_scores))),
       len(list(filter(lambda x: x == 0, polarity_scores))),
       len(list(filter(lambda x: x > 0, polarity_scores)))
       ])
ax.set_title('Sentiment Analysis of Tweets')
ax.set_xlabel('Sentiment')
ax.set_ylabel('Number of Tweets')
plt.show()

#a new dataframe 
results = pd.DataFrame(columns=['Text', 'Sentiment', 'Sentiment Score'])

#stemming+lemmmatization
for heheh in cleaned:
    text = heheh
    # Perform sentiment analysis using TextBlob
    blob = TextBlob(text)
    sentiment_score = blob.sentiment.polarity
    # Classify sentiment as positive, negative, or neutral
    if sentiment_score > 0:
        sentiment = 'positive'
    elif sentiment_score < 0:
        sentiment = 'negative'
    else:
        sentiment = 'neutral'
    # Add results to DataFrame
    results = results.append({'Text': text, 'Sentiment': sentiment, 'Sentiment Score': sentiment_score}, ignore_index=True)

# Print results
print(results)

results=results.head(15)
results

def labels(sent):
    dict={}
    for sentence in sent:
        value= TextBlob(sentence).polarity
        if value >0:
            dict[sentence]="positive"
        elif value ==0:
            dict[sentence]="neutral"
        else:
            dict[sentence]="negative"
    return dict

listt2 = labels(listt)

TB_Data= pd.DataFrame.from_dict(listt2, orient ='index')
TB_Data.rename(columns ={0:"Sentimental Analysis"})

pip install vaderSentiment

from collections import defaultdict

df.head(5)

from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm
nltk.download('vader_lexicon')
SIA=SentimentIntensityAnalyzer()

!pip install transformers
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax

#Load the pre-trained model
MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

# def roberta_polarity_scores(sent):
#     encoded_text = tokenizer(sent, return_tensors='pt')
#     output = model(**encoded_text)
#     scores = output[0][0].detach().numpy()
#     scores = softmax(scores)
#     scores_dict = {
#         'roberta_neg' : scores[0],
#         'roberta_neu' : scores[1],
#         'roberta_pos' : scores[2]
#     }
#     return scores_dict
    
# res = {}
# for i, row in tqdm(df.iterrows(), total=len(df)):
#     try:
#         text = row['text']
#         # myid = row['Id']
#         vader_result = SIA.polarity_scores(text)
#         vader_result_rename = {}
#         for key, value in vader_result.items():
#             vader_result_rename[f"vader_{key}"] = value
#         roberta_result = roberta_polarity_scores(text)
#         both = {**vader_result_rename, **roberta_result}
#     #     res[myid] = both
#     # except RuntimeError:
#     #     print(f'Broke at id {myid}')


# results_df = pd.DataFrame(res).T
# results_df = results_df.reset_index().rename(columns={'index': 'Id'})
# results_df = results_df.merge(df, how='left')

"""## 5. Feature Engineering"""

def sent_length(text):
    return len(text)

def count_words(text):
    return len(text.split())

def count_chars(text):
    return len(text)

def count_I(text):
    x = re.findall(r'i\w*', text)
    return len(x) 

def white_space(text):
    x = re.findall(r'\s', text)
    return len(x)

def count_mentions(text):
    x = re.findall(r'(@w[A-Za-z0-9]*)', text)
    return len(x)

def clean_text(text):
    _steps = [
    sent_length,
    count_words,
    count_chars,
    count_I,
    white_space,
    count_mentions,
    ]
    for step in _steps:
        text=step(text)
    return text

print(clean_text)

"""### 6. Vectorization
1 - CountVectorizer

2 - TF-IDF

3 - Word2Vec model
"""

from sklearn.feature_extraction.text import CountVectorizer


vectorizer = CountVectorizer()
vectorizer.fit_transform([cleaned])
vocabulary = vectorizer.get_feature_names_out()
bow = vectorizer.transform([cleaned])


bow_array = bow.toarray()

print("Vocabulary Size:", len(vocabulary))
print("BOW Shape:", bow_array.shape)
print("BOW Representation:\n", bow_array)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
vectorizer.fit([cleaned])
tfidf = vectorizer.transform([cleaned])


tfidf_array = tfidf.toarray()

print("TF-IDF Shape:", tfidf_array.shape)
print("TF-IDF Representation:\n", tfidf)

nltk.download('abc')







import matplotlib.pyplot as plt

#TEXTBLOB ON STEMMING+LEMMA
hehe=cleaned
# Perform sentiment analysis on each tweet
polarity_scores = []
for text in hehe:
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    polarity_scores.append(polarity)

# Plot the sentiment scores as a bar chart
fig, ax = plt.subplots()
ax.bar(['Negative', 'Neutral', 'Positive'], [
       len(list(filter(lambda x: x < 0, polarity_scores))),
       len(list(filter(lambda x: x == 0, polarity_scores))),
       len(list(filter(lambda x: x > 0, polarity_scores)))
       ])
ax.set_title('Sentiment Analysis of Tweets')
ax.set_xlabel('Sentiment')
ax.set_ylabel('Number of Tweets')
plt.show()

#sentiment analysis with lemmatization
import matplotlib.pyplot as plt

#textblob with lemmatization 
hehe=cleaned
# Perform sentiment analysis on each tweet
polarity_scores = []
for text in hehe:
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    polarity_scores.append(polarity)

# Plot the sentiment scores as a bar chart
fig, ax = plt.subplots()
ax.bar(['Negative', 'Neutral', 'Positive'], [
       len(list(filter(lambda x: x < 0, polarity_scores))),
       len(list(filter(lambda x: x == 0, polarity_scores))),
       len(list(filter(lambda x: x > 0, polarity_scores)))
       ])
ax.set_title('Sentiment Analysis of Tweets')
ax.set_xlabel('Sentiment')
ax.set_ylabel('Number of Tweets')
plt.show()

# #stemming

# results_with_stemming = pd.DataFrame(columns=['Text', 'Sentiment', 'Sentiment Score'])
# for text in cleaned:
#     text = text
#     # Perform sentiment analysis using TextBlob
#     blob = TextBlob(text)
#     sentiment_score = blob.sentiment.polarity
#     # Classify sentiment as positive, negative, or neutral
#     if sentiment_score > 0:
#         sentiment = 'positive'
#     elif sentiment_score < 0:
#         sentiment = 'negative'
#     else:
#         sentiment = 'neutral'
#     # Add results to DataFrame
#     results_with_stemming = results_with_stemming.append({'Text': text, 'Sentiment': sentiment, 'Sentiment Score': sentiment_score}, ignore_index=True)

# # Print results
# print(results_with_stemming)

# #lemmatization


# results_with_lemma = pd.DataFrame(columns=['Text', 'Sentiment', 'Sentiment Score'])
# for tweet in tweets:
#     text = tweet
#     # Perform sentiment analysis using TextBlob
#     blob = TextBlob(text)
#     sentiment_score = blob.sentiment.polarity
#     # Classify sentiment as positive, negative, or neutral
#     if sentiment_score > 0:
#         sentiment = 'positive'
#     elif sentiment_score < 0:
#         sentiment = 'negative'
#     else:
#         sentiment = 'neutral'
#     # Add results to DataFrame
#     results_with_lemma = results_with_lemma.append({'Text': text, 'Sentiment': sentiment, 'Sentiment Score': sentiment_score}, ignore_index=True)

# # Print results
# print(results_with_lemma)

from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

"""The "results" was not working above, it was taking a lot of time to run due to which could not assign X and y correctly and hence, the following code is not able to run"""

X = results['Text']
y = results["Sentiment"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

from sklearn.feature_extraction.text import CountVectorizer

# Create a pipeline
pipeline = Pipeline([
    ('count_vectorizer', CountVectorizer()),
    ('MultinomialNB',MultinomialNB())
])

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# Predict on the testing data
y_pred = pipeline.predict(X_test)

# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

pipe = Pipeline([('CV', CountVectorizer()), ('Random Forest', RandomForestClassifier(n_estimators=50, criterion="entropy"))])


# Fit the pipeline on the training data
pipe.fit(X_train, y_train)

# Predict on the testing data
y_pred = pipe.predict(X_test)

# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Create a pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

pipeline3 = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('MultinomialNB',MultinomialNB())
])
# Fit the pipeline on the training data
pipeline3.fit(X_train, y_train)

# Predict on the testing data
y_pred = pipeline.predict(X_test)

# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

pipe = Pipeline([('tfidf', TfidfVectorizer()), ('Random Forest', RandomForestClassifier(n_estimators=50, criterion="entropy"))])


# Fit the pipeline on the training data
pipe.fit(X_train, y_train)

# Predict on the testing data
y_pred = pipe.predict(X_test)

# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))